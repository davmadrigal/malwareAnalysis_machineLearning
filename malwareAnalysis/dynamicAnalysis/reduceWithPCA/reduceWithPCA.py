import pandas as pd
import numpy as np
import os
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# This script reads a CSV file, processes the data using PCA, 
# and visualizes the explained variance and feature contributions.

def read_csv_file(filename):
    """Read the CSV file and return a DataFrame."""
    return pd.read_csv(filename + '.csv')

def process_columns(df, filename):
    """Process columns based on the filename condition."""
    if 'apistats' in filename:
        preserved_columns = df.iloc[:, :3]  # Keep columns 0, 1, and 2
        df = df.drop(df.columns[[0, 1]], axis=1)  # Drop columns 0 and 1
        target = df.iloc[:, 0]  # Define the target column
        df = df.drop(df.columns[0], axis=1)  # Drop the target column
        return preserved_columns, df, target
    return None, df, None

def normalize_data(df):
    """Normalize the data using StandardScaler."""
    scaler = StandardScaler()
    return scaler.fit_transform(df)

def apply_pca(X_scaled):
    """Apply PCA to the scaled data and return the reduced dataset."""
    pca = PCA(n_components=0.95)  # Keep 95% of the variance
    return pca.fit_transform(X_scaled), pca

def save_reduced_data(reduced_data, preserved_columns, filename, target):
    """Save the reduced dataset to a CSV file."""
    if target is not None:
        result_final = pd.concat([preserved_columns.reset_index(drop=True), pd.DataFrame(reduced_data)], axis=1)
        result_final.to_csv(filename + 'reduced.csv', index=False)
    else:
        pd.DataFrame(reduced_data).to_csv(filename + 'reduced.csv', index=False)

def plot_variance_explained(pca, df, filename):
    """Plot the explained variance and feature contributions."""
    explained_variance = pca.explained_variance_ratio_
    components = pca.components_

    # Plot explained variance
    plt.figure(figsize=(10, 6))
    plt.plot(np.cumsum(explained_variance), linewidth=3)
    plt.xlabel('Number of Components')
    plt.ylabel('Cumulative Explained Variance')
    plt.title('Explained Variance by PCA')
    plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')
    plt.grid(True)
    plt.legend()
    plt.savefig("./reducedfiles/images/" + filename + '_varianza.png')
    plt.show()

    # Plot feature contributions
    features = df.columns
    contributions = pd.DataFrame(components, columns=features)
    top_n = 20
    top_features = contributions.iloc[0].abs().nlargest(top_n).index

    plt.figure(figsize=(12, 8))
    plt.barh(top_features, contributions.iloc[0][top_features], color='skyblue')
    plt.xlabel('Contribution')
    plt.title('Top 20 Features Contributing to the First Principal Component')
    plt.axvline(x=0, color='grey', linestyle='--')
    plt.savefig("./reducedfiles/images/" + filename + '.png')
    plt.show()

def get_filenames_from_directory(directory):
    """Get a list of filenames without the .csv extension from the specified directory."""
    return [f[:-4] for f in os.listdir(directory) if f.endswith('.csv')]

def main():
    # Create directories if they do not exist
    os.makedirs('./reducedfiles/images', exist_ok=True)

     # Get filenames from the directory
    directory = './splitFiles/'  # Specify the directory where the CSV files are located
    filenames = get_filenames_from_directory(directory)
    
    for filename in filenames:
        print(filename)
        df = read_csv_file(directory + filename)
        preserved_columns, df, target = process_columns(df, filename)
        X_scaled = normalize_data(df)
        reduced_data, pca = apply_pca(X_scaled)
        save_reduced_data(reduced_data, preserved_columns, "./reducedfiles/" + filename, target)
        print(f"The file has been reduced and saved as ./reducedFiles/{filename}reduced.csv")
        plot_variance_explained(pca, df, filename)

if __name__ == "__main__":
    main()

