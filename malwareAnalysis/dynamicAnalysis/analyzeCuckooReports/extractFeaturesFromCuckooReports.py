import os
import json
from natsort import natsorted

# This script processes JSON report files generated by Cuckoo Sandbox analyses.
# It extracts relevant features such as dropped files and API statistics,
# and writes them to an output file while ensuring uniqueness of entries.

def process_report_json(base_path):
    """Process JSON report files and extract relevant features."""
    # Output file
    with open('features.txt', 'w') as output_file:
        # Iterate over directories in the base path
        for directory_name in natsorted(os.listdir(base_path)):
            directory_path = os.path.join(base_path, directory_name)
            
            # Check if it is a directory and if its name is numeric
            if os.path.isdir(directory_path) and directory_name.isdigit():
                report_path = os.path.join(directory_path, 'reports', 'report.json')

                print("Analyzing: ", report_path)
                
                # Check if the report.json file exists
                if os.path.isfile(report_path):
                    with open(report_path, 'r') as file:
                        try:
                            # Load the content of the JSON file
                            json_content = json.load(file)
                            
                            # Extract values from 'dropped'
                            extract_dropped_files(json_content, output_file)

                            # Extract values from 'apistats'
                            extract_api_stats(json_content, output_file)

                            # Extract values from 'summary'
                            extract_summary(json_content, output_file)
                            
                        except Exception as e: 
                            print(f'Error decoding JSON in {report_path}: {e}')
                else:
                    print(f'No report.json file found in {report_path}')
            else:
                print(f'{directory_path} is not a numeric directory.')

    # Remove duplicates from the output file
    remove_duplicates('features.txt')

    # Add "name" and "family" rows at the beginning (if needed)
    # add_rows_at_beginning('features.txt', 'name', 'family')

def extract_dropped_files(json_content, output_file):
    """Extract dropped files from the JSON content."""
    if 'dropped' in json_content:
        dropped_items = json_content['dropped']
        # Ensure dropped_items is a list
        if isinstance(dropped_items, list):
            for item in dropped_items:
                try:
                    filename = item.get('filepath', 'unknown')  # Use get to avoid KeyError
                    output_file.write(f'dropped: {filename}\n')
                except Exception as e:
                    print(f'Error processing item in dropped: {e}')
        elif isinstance(dropped_items, dict):  # If it's a single object, convert to list
            filename = dropped_items.get('filepath', 'unknown')
            output_file.write(f'dropped: {filename}\n')

def extract_api_stats(json_content, output_file):
    """Extract API statistics from the JSON content."""
    if 'behavior' in json_content and 'apistats' in json_content['behavior']:
        apistats = json_content['behavior']['apistats']
        for key, values in apistats.items():
            if isinstance(values, dict):
                for name, value in values.items():
                    output_file.write(f'apistats: {name}\n')

def extract_summary(json_content, output_file):
    """Extract summary information from the JSON content."""
    if 'behavior' in json_content and 'summary' in json_content['behavior']:
        summary = json_content['behavior']['summary']
        for field, values in summary.items():
            if isinstance(values, list):
                for value in values:
                    output_file.write(f'{field}: {value}\n')

def remove_duplicates(filename):
    """Remove duplicate lines from the specified file."""
    unique_lines = set()
    
    with open(filename, 'r') as file:
        for line in file:
            unique_lines.add(line.strip())
    
    with open(filename, 'w') as file:
        for line in sorted(unique_lines):
            file.write(f'{line}\n')

def add_rows_at_beginning(filename, name, family):
    """Add specified rows at the beginning of the file."""
    # Read the current content of the file
    with open(filename, 'r') as file:
        lines = file.readlines()
    
    # Write the new rows at the beginning
    with open(filename, 'w') as file:
        file.write(f'1: {name}\n')
        file.write(f'2: {family}\n')
        file.writelines(lines)

# Base path where the analyses are located
base_path = '/home/daleal/.cuckoo/storage/analyses'
process_report_json(base_path)

